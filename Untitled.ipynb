{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 1. Add the Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk import pos_tag\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import WordNetLemmatizer\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from collections import defaultdict\n",
    "from nltk.corpus import wordnet as wn\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn import model_selection, naive_bayes, svm\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     C:\\Users\\sharm\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP -2: Set random seed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.random.seed(500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 3: Read Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "####### function for reading dataset \n",
    "import re , datetime\n",
    "import sklearn.datasets as skd\n",
    "import pandas as pd\n",
    "import nltk\n",
    "# import StanfordTokenizer() method from nltk \n",
    "#from nltk.tokenize.stanford import StanfordTokenizer \n",
    "\n",
    "def read_datasets():    \n",
    "    categories = ['alt.atheism', 'soc.religion.christian','comp.graphics', 'sci.med']\n",
    "    #categories = ['sci.med']\n",
    "    c=len(categories)\n",
    "    train = skd.load_files('C:\\\\Users\\\\sharm\\\\Documents\\\\Iswc2020Matcher\\\\News20\\\\data\\\\20news-bydate-train', categories= categories, encoding= 'ISO-8859-1')\n",
    "    test = skd.load_files('C:\\\\Users\\\\sharm\\\\Documents\\\\Iswc2020Matcher\\\\News20\\\\data\\\\20news-bydate-test',categories= categories, encoding= 'ISO-8859-1')\n",
    "    train = pd.DataFrame({'data': train.data, 'target': train.target})\n",
    "    test = pd.DataFrame({'data': test.data, 'target': test.target})\n",
    "    return train,test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "train,test = read_datasets()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Step 4: Preprocessing \n",
    "\n",
    "    Remove Blank rows in Data, if any \n",
    "    Change all the text to lower case  \n",
    "    Word Tokenization   \n",
    "    Remove Stop words\n",
    "    Remove Non-alpha text\n",
    "    Word Lemmatization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - a : Remove blank rows if any.\n",
    "train['data'].dropna(inplace=True)\n",
    "test['data'].dropna(inplace=True)\n",
    "\n",
    "# Step - b : Change all the text to lower case. This is required as python interprets 'dog' and 'DOG' differently\n",
    "train['data'] = [entry.lower() for entry in train['data']]\n",
    "test['data'] = [entry.lower() for entry in test['data']]\n",
    "\n",
    "# Step - c : Tokenization : In this each entry in the corpus will be broken into set of words\n",
    "train['data']= [word_tokenize(entry) for entry in train['data']]\n",
    "test['data']= [word_tokenize(entry) for entry in test['data']]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. \n",
    "#By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(train['data']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    train.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[from, :, dpc47852, @, uxa.cso.uiuc.edu, (, da...</td>\n",
       "      <td>2</td>\n",
       "      <td>['daniel', 'paul', 'checkman', 'subject', 'msg...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[from, :, yoo, @, engr.ucf.edu, (, hoi, yoo, )...</td>\n",
       "      <td>1</td>\n",
       "      <td>['yoo', 'hoi', 'yoo', 'subject', 'look', 'usa'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[from, :, fernandeza, @, merrimack.edu, subjec...</td>\n",
       "      <td>3</td>\n",
       "      <td>['fernandeza', 'subject', 'arrogance', 'christ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[from, :, mcelwre, @, cnsvax.uwec.edu, subject...</td>\n",
       "      <td>2</td>\n",
       "      <td>['mcelwre', 'subject', 'natural', 'remedy', 'o...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[from, :, mathew, &lt;, mathew, @, mantis.co.uk, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>['mathew', 'mathew', 'subject', 'inimitable', ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  target  \\\n",
       "0  [from, :, dpc47852, @, uxa.cso.uiuc.edu, (, da...       2   \n",
       "1  [from, :, yoo, @, engr.ucf.edu, (, hoi, yoo, )...       1   \n",
       "2  [from, :, fernandeza, @, merrimack.edu, subjec...       3   \n",
       "3  [from, :, mcelwre, @, cnsvax.uwec.edu, subject...       2   \n",
       "4  [from, :, mathew, <, mathew, @, mantis.co.uk, ...       0   \n",
       "\n",
       "                                          text_final  \n",
       "0  ['daniel', 'paul', 'checkman', 'subject', 'msg...  \n",
       "1  ['yoo', 'hoi', 'yoo', 'subject', 'look', 'usa'...  \n",
       "2  ['fernandeza', 'subject', 'arrogance', 'christ...  \n",
       "3  ['mcelwre', 'subject', 'natural', 'remedy', 'o...  \n",
       "4  ['mathew', 'mathew', 'subject', 'inimitable', ...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step - d : Remove Stop words, Non-Numeric and perfom Word Stemming/Lemmenting.\n",
    "# WordNetLemmatizer requires Pos tags to understand if the word is noun or verb or adjective etc. \n",
    "#By default it is set to Noun\n",
    "tag_map = defaultdict(lambda : wn.NOUN)\n",
    "tag_map['J'] = wn.ADJ\n",
    "tag_map['V'] = wn.VERB\n",
    "tag_map['R'] = wn.ADV\n",
    "for index,entry in enumerate(test['data']):\n",
    "    # Declaring Empty List to store the words that follow the rules for this step\n",
    "    Final_words = []\n",
    "    # Initializing WordNetLemmatizer()\n",
    "    word_Lemmatized = WordNetLemmatizer()\n",
    "    # pos_tag function below will provide the 'tag' i.e if the word is Noun(N) or Verb(V) or something else.\n",
    "    for word, tag in pos_tag(entry):\n",
    "        # Below condition is to check for Stop words and consider only alphabets\n",
    "        if word not in stopwords.words('english') and word.isalpha():\n",
    "            word_Final = word_Lemmatized.lemmatize(word,tag_map[tag[0]])\n",
    "            Final_words.append(word_Final)\n",
    "    # The final processed set of words for each iteration will be stored in 'text_final'\n",
    "    test.loc[index,'text_final'] = str(Final_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>data</th>\n",
       "      <th>target</th>\n",
       "      <th>text_final</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>[from, :, koberg, @, spot.colorado.edu, (, all...</td>\n",
       "      <td>3</td>\n",
       "      <td>['koberg', 'allen', 'koberg', 'subject', 'bibl...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>[from, :, bobbe, @, vice.ico.tek.com, (, rober...</td>\n",
       "      <td>0</td>\n",
       "      <td>['bobbe', 'robert', 'beauchaine', 'subject', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>[from, :, joe, @, erix.ericsson.se, (, joe, ar...</td>\n",
       "      <td>3</td>\n",
       "      <td>['joe', 'joe', 'armstrong', 'subject', 'angel'...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>[from, :, sdl, @, linus.mitre.org, (, steven, ...</td>\n",
       "      <td>2</td>\n",
       "      <td>['sdl', 'steven', 'litvintchouk', 'subject', '...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>[from, :, kxgst1+, @, pitt.edu, (, kenneth, gi...</td>\n",
       "      <td>2</td>\n",
       "      <td>['kenneth', 'gilbert', 'subject', 'pregnency',...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                data  target  \\\n",
       "0  [from, :, koberg, @, spot.colorado.edu, (, all...       3   \n",
       "1  [from, :, bobbe, @, vice.ico.tek.com, (, rober...       0   \n",
       "2  [from, :, joe, @, erix.ericsson.se, (, joe, ar...       3   \n",
       "3  [from, :, sdl, @, linus.mitre.org, (, steven, ...       2   \n",
       "4  [from, :, kxgst1+, @, pitt.edu, (, kenneth, gi...       2   \n",
       "\n",
       "                                          text_final  \n",
       "0  ['koberg', 'allen', 'koberg', 'subject', 'bibl...  \n",
       "1  ['bobbe', 'robert', 'beauchaine', 'subject', '...  \n",
       "2  ['joe', 'joe', 'armstrong', 'subject', 'angel'...  \n",
       "3  ['sdl', 'steven', 'litvintchouk', 'subject', '...  \n",
       "4  ['kenneth', 'gilbert', 'subject', 'pregnency',...  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP -5: Prepare Train and Test Data sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "Train_X = train.text_final\n",
    "Test_X  = test.text_final\n",
    "Train_Y = train.target\n",
    "Test_Y  = test.target"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP -6: Encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "Encoder = LabelEncoder()\n",
    "Train_Yq = Encoder.fit_transform(Train_Y)\n",
    "Test_Yq = Encoder.fit_transform(Test_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP -7: Word Vectorization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "Tfidf_vect = TfidfVectorizer(max_features=5000)\n",
    "Tfidf_vect.fit(train['text_final'])\n",
    "Train_X_Tfidf = Tfidf_vect.transform(Train_X)\n",
    "Test_X_Tfidf = Tfidf_vect.transform(Test_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Tfidf_vect.vocabulary_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#print(Train_X_Tfidf)\n",
    "#(0, 4978) 0.013641666546980962\n",
    "#1. Row number of ‘Train_X_Tfidf’, \n",
    "#2: Unique Integer number of each word in the first row, \n",
    "#3: Score calculated by TF-IDF Vectorizer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "STEP -8: Use the ML Algorithms to Predict the outcome"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Naive Bayes Accuracy Score ->  88.41544607190413\n"
     ]
    }
   ],
   "source": [
    "# fit the training dataset on the NB classifier\n",
    "Naive = naive_bayes.MultinomialNB()\n",
    "Naive.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_NB = Naive.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "print(\"Naive Bayes Accuracy Score -> \",accuracy_score(predictions_NB, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SVM Accuracy Score ->  89.61384820239681\n"
     ]
    }
   ],
   "source": [
    "# Classifier - Algorithm - SVM\n",
    "# fit the training dataset on the classifier\n",
    "SVM = svm.SVC(C=1.0, kernel='linear', degree=3, gamma='auto')\n",
    "SVM.fit(Train_X_Tfidf,Train_Y)# predict the labels on validation dataset\n",
    "predictions_SVM = SVM.predict(Test_X_Tfidf)# Use accuracy_score function to get the accuracy\n",
    "print(\"SVM Accuracy Score -> \",accuracy_score(predictions_SVM, Test_Y)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word2vec embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "sentences = [[\"cat\", \"say\", \"meow\"], [\"dog\", \"say\", \"woof\"]]\n",
    "model = Word2Vec(sentences, min_count=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "vector = model.wv['cat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 2.6889052e-03, -3.2475619e-03,  4.4311364e-03,  4.7342293e-03,\n",
       "        2.3851776e-03,  5.2145694e-04, -3.2955538e-03, -3.9260350e-03,\n",
       "        1.0875356e-03, -1.6195885e-03, -1.6356009e-03, -3.9639357e-03,\n",
       "        3.2271182e-06,  1.2043347e-03,  3.7724508e-03, -1.8608301e-04,\n",
       "       -9.0082060e-04,  3.5956078e-03, -2.4978491e-03, -4.2991722e-03,\n",
       "        5.2830845e-04, -2.4395101e-03, -4.4112029e-03,  9.1889451e-05,\n",
       "        2.7890888e-03, -7.4143399e-04, -2.2253366e-03,  5.4945255e-04,\n",
       "        4.2316802e-03,  1.1988498e-03, -2.7603793e-03,  3.6878963e-03,\n",
       "       -4.2051692e-03, -3.0902606e-03,  1.2108495e-03,  4.3116491e-03,\n",
       "        3.1942038e-03,  5.2693096e-04, -2.5778159e-03,  1.8131731e-03,\n",
       "       -2.3540966e-03, -1.0483249e-03, -1.6874263e-04,  1.6546038e-03,\n",
       "       -2.2740050e-03, -4.2057489e-03,  3.4335261e-04,  3.4583043e-03,\n",
       "       -2.7334466e-04, -1.3224296e-03, -7.7152258e-04, -4.4255477e-04,\n",
       "       -1.4621363e-03,  7.7722181e-04, -4.9434323e-03,  4.8588105e-03,\n",
       "        5.0441181e-06,  8.5160305e-04, -9.7173837e-04,  3.6795526e-03,\n",
       "       -7.3752028e-04, -2.5608374e-03,  6.6723797e-04, -5.5320310e-05,\n",
       "        5.2713288e-04,  4.1196048e-03, -3.0678641e-03, -2.5227359e-03,\n",
       "        1.9264515e-03, -4.5894179e-03, -2.0320183e-03,  9.0072019e-04,\n",
       "       -3.3281480e-03,  2.8833223e-03, -2.5761914e-03,  4.7724409e-04,\n",
       "        8.7498251e-04,  1.8858983e-03, -1.0269325e-03,  1.6060164e-03,\n",
       "        4.5992224e-03, -5.5008254e-04,  3.6137451e-03, -2.7138193e-03,\n",
       "       -1.6964601e-03,  4.6805623e-03, -2.9041586e-03,  3.1141069e-04,\n",
       "       -4.6896208e-03, -2.4410742e-03, -4.4883523e-04, -3.2784354e-03,\n",
       "       -1.8756869e-03,  4.1567092e-03, -4.2422023e-03,  3.0154865e-03,\n",
       "        1.5556881e-03, -2.4974640e-04,  8.8726978e-05, -4.8438190e-03],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
